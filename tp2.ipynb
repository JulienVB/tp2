{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 - Julien Vanbelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import os\n",
    "import yake\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import webtext\n",
    "nltk.download('stopwords')\n",
    "nltk.download('webtext')\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from tabulate import tabulate\n",
    "## variables\n",
    "year = 1890\n",
    "data_path = \"/Users/julienvanbelle/Documents/GitHub/tac/data/txt\" ## /!\\ à changer \n",
    "temp_path = '/Users/julienvanbelle/Documents/GitHub/tac/data'     ## /!\\ à changer\n",
    "all_path = \"/Users/julienvanbelle/Documents/GitHub/tac/data/all.txt\" ## /!\\ à changer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction des mots clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enlever les fichiers qui ne commencent pas par Bxl_1890\n",
    "files = os.listdir(data_path)\n",
    "date_files = [f for f in files if f.startswith('Bxl_1890')]\n",
    "len(date_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le contenu de ces fichiers dans une liste\n",
    "content_list = []\n",
    "for txt in date_files:\n",
    "    with open(os.path.join(data_path, txt), 'r') as f:\n",
    "        content_list.append(f.read())\n",
    "\n",
    "len(content_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w') as f:\n",
    "    f.write(' '.join(content_list))\n",
    "    print(\"temp file saved in\",temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de la fréquence des mots pour définir les stop word et la limite de récurrence à appliquer pour les classer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_words = webtext.words(\"/Users/julienvanbelle/Documents/GitHub/tac/data/1890.txt\")\n",
    "data_analysis = nltk.FreqDist(wt_words)\n",
    " \n",
    "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
    "\n",
    "##for key in sorted(filter_words):\n",
    "    ##print(\"%s: %s\" % (key, filter_words[key]))\n",
    " \n",
    "data_analysis = nltk.FreqDist(filter_words)\n",
    "data_analysis.plot(30, cumulative=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une liste de stop words en fonction de la fréquence des mots dans les documents de l'année selectionnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addsw = []\n",
    "for z in sorted(filter_words):\n",
    "  if filter_words[z] > 200:\n",
    "   ##print(\"%s: %s\" % (z, filter_words[z]))\n",
    "   addsw.append(z)\n",
    "\n",
    "print(addsw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += addsw\n",
    "sw += \"conseil communal\", \"conseil général\", \"conseil supérieur\", \"administration communale\", \"conseil provincial\", \"l'administration communale\", \"conseil\", \"echevin\" , \"messieurs\"\n",
    "sw = set(sw)\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'extraction des mots clés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sorted(date_files):\n",
    "    text = open(os.path.join(data_path, f), 'r').read()\n",
    "    keywords = kw_extractor.extract_keywords(text.lower())\n",
    "    kept = []\n",
    "    for kw, score in keywords:\n",
    "        words = kw.split()\n",
    "        if len(words) == 2 and kw.lower() not in sw:\n",
    "            kept.append(kw)\n",
    "    print(f\"{f} mentions these keywords: {', '.join(kept)}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'r') as f:\n",
    "    before = f.read()\n",
    "\n",
    "before[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction de nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.lower() for w in words if len(w) > 2 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(year, folder=temp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier le résultat\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r') as f:\n",
    "    after = f.read()\n",
    "\n",
    "after[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = Counter(after.split())\n",
    "print(frequencies.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche des entités nommées principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=900000\n",
    "text = open(all_path, encoding='utf-8').read()[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les entités\n",
    "people = defaultdict(int)\n",
    "organisation = defaultdict(int)\n",
    "places = defaultdict(int)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1\n",
    "    if ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "        organisation[ent.text] += 1\n",
    "    if ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "        places[ent.text] += 1\n",
    "      \n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trier et imprimer\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_organisation = sorted(organisation.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_places = sorted(places.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "\n",
    "for person, freq in sorted_people:\n",
    "    print(f\"{person} (PER)apparait {freq} fois dans le corpus\")\n",
    "\n",
    "for organisation, freq in sorted_organisation:\n",
    "    print(f\"{organisation} (ORG)apparait {freq} fois dans le corpus\")\n",
    "\n",
    "for places, freq in sorted_places:\n",
    "    print(f\"{places} (LOC)apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = []\n",
    "\n",
    "phrase1 = \"C'est dire que vous avez des facultés et des aptitudes spéciales pour l'art auquel vous avez consacré votre vie.\"\n",
    "phrase2 = \"Les artistes musiciens et choristes du théâtre de l'Alhambra sollicitent un subside à l'effet de pouvoir organiser des concerts classiques et hebdomadaires.\"\n",
    "phrase3 = \"M. Yseux. J'ai posé la question de savoir si le fait est vrai ou faux.\"\n",
    "phrase4 = \"Dans ces conditions, nous espérons que la Députation permanente refusera de mandater d'office une dépense qui légalement nenous incombe pas. Au besoin, la justice sera appelée à statuer.\"\n",
    "phrase5 = \"Le directeur fait, à la fin de chaque mois, le dépouillement des certificats délivrés par les chefs de service aux internes et aux externes et le transmet au Secrétariat général.\"\n",
    "phrase6 = \"Il prépare les médicaments nécessaires et surveille l'administration de ceux-ci ; il ne peut modifier en rien les traitements indiqués par le médecin directeur, ni en instituer aucun autre, lorsque celui-ci est à l'hospice.\"\n",
    "phrase7 = \"Dans la dernière séance, nous avons voté poulies fêtes un crédit supplémentaire de 53,000 francs.\"\n",
    "phrase8 = \"Il était néanmoins difficile de la combler parce que, d'une part, les crédits votés par le Conseil communal devaient subvenir à de multiples emplois, et que, d'autre part, le crédit voté par les Chambres était des plus réduits.\"\n",
    "phrase9 = \"Nous ne pouvons mieux faire c o n n a î t r e la situation p r o s p è r e de l'Académie qu'en mentionnant le nombre d'élèves q u i ont é t é inscrits pour les cours d u soir, soit 7 5 9 , ce q u i , avec les 259 jeunes gens inscrits pour les cours du j o u r , forme un total de 1,018 élèves ; beaucoup d'entre eux se sont fait inscrire à la fois pour les cours du jour et du soir.\"\n",
    "phrase10 = \"Grâce à la généreuse intervention du fonds Bischoffsheim, l'ouverture de l'école a eu lieu le 9 septembre 1889.\"\n",
    "phrase_list.append(phrase1)\n",
    "phrase_list.append(phrase2)\n",
    "phrase_list.append(phrase3)\n",
    "phrase_list.append(phrase4)\n",
    "phrase_list.append(phrase5)\n",
    "phrase_list.append(phrase6)\n",
    "phrase_list.append(phrase7)\n",
    "phrase_list.append(phrase8)\n",
    "phrase_list.append(phrase9)\n",
    "phrase_list.append(phrase10)\n",
    "\n",
    "##print(phrase_list[:10])\n",
    "print(\"nb d'éléments dans la liste =\",len(phrase_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction d'analyse de sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "score_list = [['Phrase','Polarity', 'Subjectivity']]\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")\n",
    "    score_list.append([y+1,polarity_str,subjectivity_str])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des 10 phrases choisies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for y in range(0,len(phrase_list)):\n",
    " print(\"Phrase\", y+1, \"==\" ,phrase_list[y])\n",
    " get_sentiment(phrase_list[y])\n",
    " print()\n",
    " print(score_list[y+1])\n",
    " print()\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un tableau récapitulatif de la polarité et de la subjectivité de chaque phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabulate(score_list,headers='firstrow',tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
